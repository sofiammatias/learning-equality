{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sofiamatias/learning-equality-challenge-semanticsearch?scriptVersionId=121602210\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import sys, os\nsys.path.append(\"../input/sentence-transformer-package/sentence-transformers-2.2.2/sentence-transformers-2.2.2\") \nimport sentence_transformers\nfrom sentence_transformers import SentenceTransformer, CrossEncoder, util","metadata":{"execution":{"iopub.status.busy":"2023-03-09T09:50:13.323253Z","iopub.execute_input":"2023-03-09T09:50:13.324178Z","iopub.status.idle":"2023-03-09T09:50:30.121121Z","shell.execute_reply.started":"2023-03-09T09:50:13.324113Z","shell.execute_reply":"2023-03-09T09:50:30.119289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport string\nimport torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-09T09:50:30.124195Z","iopub.execute_input":"2023-03-09T09:50:30.125626Z","iopub.status.idle":"2023-03-09T09:50:30.132778Z","shell.execute_reply.started":"2023-03-09T09:50:30.125567Z","shell.execute_reply":"2023-03-09T09:50:30.131398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading dataframes","metadata":{}},{"cell_type":"code","source":"challenge_files_path = '/kaggle/input/learning-equality-curriculum-recommendations'\nprivate_files_path = '/kaggle/input/learningequalityfiles'\nmodel_files_path = '/kaggle/input/sentence-transformer-package'\n\nprint (f\"\\nLoading dataframes...\")\n\nfor dirname, _, filenames in os.walk(challenge_files_path):\n    for filename in filenames:\n        filepath = os.path.join(dirname, filename)\n        print (f\"\\nLoading dataframe from {filepath}...\")\n        df = pd.read_csv (filepath)\n        if 'topics' in filepath:\n            topics_df = df.fillna({\"title\": \"\", \"description\": \"\"})\n            display(topics_df)\n        elif 'sample_submission' in filepath:\n            print (f\"\\nLoading 'sample' dataframe...\")\n            sample_df = df\n            display(sample_df)\n        elif 'correlations' in filepath:\n            correlations_df = df.fillna({\"title\": \"\", \"description\": \"\"})\n            display(correlations_df)\n            print (f\"\\nCreating exploded correlations 'corr' dataframe\")\n            corr_df = correlations_df.copy()\n            corr_df['content_ids'] = corr_df.content_ids.str.split(' ')\n            corr_df = corr_df.explode('content_ids')\n            display (corr_df)\n        elif 'content' in filepath:\n            contents_df = df.fillna({\"title\": \"\", \"description\": \"\", \"text\": \"\"})\n            display(contents_df)\nprint (f\"\\nDataframes loaded.\")","metadata":{"execution":{"iopub.status.busy":"2023-03-09T09:50:54.840396Z","iopub.execute_input":"2023-03-09T09:50:54.840843Z","iopub.status.idle":"2023-03-09T09:51:19.44448Z","shell.execute_reply.started":"2023-03-09T09:50:54.840804Z","shell.execute_reply":"2023-03-09T09:51:19.443035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Choosing sample data","metadata":{}},{"cell_type":"code","source":"print (\"\\nDefining sampled dataset...\")\nuse_submission_sample = True\nsamples = 1000\nif ~sample_df.empty and use_submission_sample:\n    corr_df = corr_df[corr_df.topic_id.isin(sample_df.topic_id)]\n    correlations_df = correlations_df[correlations_df.topic_id.isin(sample_df.topic_id)]\n    topics_df = topics_df[topics_df.id.isin(sample_df.topic_id)]\n    samples = 5\n    print (f\"\\nFiltered 'topics' to {len(topics_df)} samples and 'contents' to {len(contents_df)} samples\")\nelse:\n    topics_df = topics_df[topics_df.has_content == True].sample(n=samples)\n    corr_df = corr_df[corr_df.topic_id.isin(topics_df.id)]\n    correlations_df = correlations_df[correlations_df.topic_id.isin(topics_df.id)]\n\ndisplay (topics_df)\ndisplay (contents_df)\ndisplay (correlations_df)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T09:51:25.64057Z","iopub.execute_input":"2023-03-09T09:51:25.641022Z","iopub.status.idle":"2023-03-09T09:51:25.688524Z","shell.execute_reply.started":"2023-03-09T09:51:25.640981Z","shell.execute_reply":"2023-03-09T09:51:25.686536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning data","metadata":{}},{"cell_type":"markdown","source":"* Remove ponctuation and special chars from text fields\n* Delete columns 'copyright_holder' and 'license' from 'contents'\n* Filter 'topics' by 'has_content' = True\n* Group 'topics' and 'contents' by language\n* Change 'level' column from numbers to text","metadata":{}},{"cell_type":"code","source":"def clean_text(text_col):\n    \"\"\"\n    Clean ponctuation and special chars from a dataframe column\n    \"\"\"\n    punctuations = string.punctuation\n    text_col = text_col.str.replace('\\W', ' ', regex=True)\n    for punct in string.punctuation:\n        text_col = text_col.str.replace(punct, ' ', regex=True)\n    return text_col.str.lower()","metadata":{"execution":{"iopub.status.busy":"2023-03-09T09:51:27.609567Z","iopub.execute_input":"2023-03-09T09:51:27.610105Z","iopub.status.idle":"2023-03-09T09:51:27.618873Z","shell.execute_reply.started":"2023-03-09T09:51:27.610055Z","shell.execute_reply":"2023-03-09T09:51:27.617415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleaning topics\nlevels = {1: 'Level 1', 2: 'Level 2', 3: 'Level 3', 4: 'Level 4', 5: 'Level 5', 6: 'Level 6', 7: 'Level 7', \n          8: 'Level 8', 9: 'Level 9', 10: 'Level 10', 0: 'Level 0'}\ntopics_cols = ['title', 'description']\n\nprint (f\"\\nCreating and cleaning topic features...\")\ntopic_features = topics_df.copy()\ntopic_features = topic_features.replace ({'level': levels})\nfor col in topics_cols:\n    topic_features[col] = clean_text(topic_features[col])\ntopic_features.sort_values (by='language', inplace=True)\n\n#topics_features['sentences'] = topics_features[topics_cols].apply(lambda x: '.'.join(x.dropna().astype(str)), axis=1)\n#topics_features = topics_features.drop(columns=['parent'] + topics_cols) \nprint (f\"\\nCreated 'topic_features'\")\ndisplay (topic_features)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T09:51:28.805841Z","iopub.execute_input":"2023-03-09T09:51:28.806356Z","iopub.status.idle":"2023-03-09T09:51:28.855757Z","shell.execute_reply.started":"2023-03-09T09:51:28.806295Z","shell.execute_reply":"2023-03-09T09:51:28.854215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleaning contents\ncontent_cols = ['title']\n\n\nprint (f\"\\nCreating and cleaning content features...\")\ncontent_features = contents_df.copy()\nfor col in content_cols:\n    content_features[col] = clean_text(content_features[col])\n#content_features['sentences'] =  content_features[content_cols].apply(lambda x: '.'.join(x.dropna().astype(str)), axis=1)\ncontent_features.sort_values (by='language', inplace=True)\ncontent_features.drop(columns=['copyright_holder', 'license'], inplace=True)\nprint (f\"\\nCreated 'content_features'\")\ndisplay(content_features)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T09:51:30.635686Z","iopub.execute_input":"2023-03-09T09:51:30.636605Z","iopub.status.idle":"2023-03-09T09:51:33.950385Z","shell.execute_reply.started":"2023-03-09T09:51:30.636554Z","shell.execute_reply":"2023-03-09T09:51:33.949008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scoring: F2 score ","metadata":{}},{"cell_type":"code","source":"def calculate_F2score(pred_df, act_df):\n    \n    \"\"\"\n    Using predictions_df and actual_df as exploded correlation columns to calculate F1 score.\n    Results show correct predicts, recall, precision and F2 score.\n    Results also return the list of correct predicts, correct_df_\n    \"\"\"\n    print ('\\nCalculating scores...')\n    if pred_df.empty or act_df.empty:\n        print ('\\nOne or both dataframes are empty. Abort F2score calculation.')\n        return None\n    prediction_df=pred_df.copy()\n    actual_df = act_df.copy()\n    prediction_df.columns=['topic_id', 'content_ids_pred']\n    actual_df.columns=['topic_id', 'content_ids_actual']\n    df = pd.merge(prediction_df, actual_df, how='inner', on='topic_id')\n    if df.empty:\n        print ('\\nNo matches between predictions and correlations. Abort F2score calculation.')\n        return None\n    df['tp'] = df[['content_ids_pred', 'content_ids_actual']].apply (lambda x: len(set(x['content_ids_actual'].split()).intersection(set(x['content_ids_pred'].split()))), axis=1)\n    df['fp'] = df[['content_ids_pred', 'content_ids_actual']].apply (lambda x: len(set(x['content_ids_pred'].split()) - set(x['content_ids_actual'].split())), axis=1)\n    df['fn'] = df[['content_ids_pred', 'content_ids_actual']].apply (lambda x: len(set(x['content_ids_actual'].split()) - set(x['content_ids_pred'].split())), axis=1)\n    df['precision'] = df['tp'] / (df['tp'] + df['fp'])\n    df['recall'] = df['tp'] / (df['tp'] + df['fn'])\n    df['f2'] = df['tp'] / (df['tp'] + 0.2 * df['fp'] + 0.8 * df['fn']) \n    print ('\\nF2 score calculation finished.')\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-03-09T09:51:34.992402Z","iopub.execute_input":"2023-03-09T09:51:34.992833Z","iopub.status.idle":"2023-03-09T09:51:35.00666Z","shell.execute_reply.started":"2023-03-09T09:51:34.992795Z","shell.execute_reply":"2023-03-09T09:51:35.004739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Getting matches: sentence transformer with retrain-rerank","metadata":{}},{"cell_type":"code","source":"def search(query, topic_embedding, corpus_embeddings, content_sentences, content_ids, cross_encoder, top_k):\n\n    # passages = content_sentences\n\n    ##### Semantic Search #####\n    # find potentially relevant passages\n    hits = util.semantic_search(topic_embedding, corpus_embeddings, top_k=top_k)\n    hits = hits[0]  # Get the hits for the first query\n\n    ##### Re-Ranking #####\n    # Now, score all retrieved passages with the cross_encoder\n    cross_inp = [[query, content_sentences[hit['corpus_id']]] for hit in hits]\n    cross_scores = cross_encoder.predict(cross_inp)\n\n    # Sort results by the cross-encoder scores\n    for idx in range(len(cross_scores)):\n        hits[idx]['cross-score'] = cross_scores[idx]\n\n    # Output of top-30 hits from re-ranker\n    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n    results = {}\n    for hit in hits[0:top_k]:\n        results[content_ids.iloc[hit['corpus_id']]] = content_sentences[hit['corpus_id']]\n    return results","metadata":{"execution":{"iopub.status.busy":"2023-03-09T09:51:36.06998Z","iopub.execute_input":"2023-03-09T09:51:36.07047Z","iopub.status.idle":"2023-03-09T09:51:36.078979Z","shell.execute_reply.started":"2023-03-09T09:51:36.070428Z","shell.execute_reply":"2023-03-09T09:51:36.077781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\ngpu_on = True\n\nif not torch.cuda.is_available():\n    print(\"Warning: No GPU found. Please add GPU to your notebook\")\n    gpu_on = False\n    \nlanguages = topic_features.language.unique()\nprint (languages)\npreds = {}\nmatches = {}\nbiencoder = \"/kaggle/input/learning-equality-st-train-sm/ST-all-MiniLM-L6-v2-trained\"\ncrossencoder = '/kaggle/input/msmarcominilml6v2/ms-marco-MiniLM-L-6-v2'\n\nprint (f\"\\nGetting matches using bi-encoder {biencoder} and cross encoder {crossencoder}...\")\n\n#Use the Bi-Encoder to encode all contents, so that we can use it with semantic search\nbi_encoder = SentenceTransformer(biencoder)\nbi_encoder.max_seq_length = 256    #Truncate long passages to 256 tokens (256 is ideal value)\ntop_k = 5                         #Number of passages we want to retrieve with the bi-encoder (10 is ideal value)\n\n#Use a cross-encoder, to re-rank the results list to improve the quality\ncross_encoder = CrossEncoder(crossencoder)\n\nfor lang in languages:\n    print ('\\nWorking on topics for language ', lang)\n    content_sentences = content_features[content_features.language == lang]\n    topic_sentences = topic_features[topic_features.language == lang]\n\n    if len(content_sentences) == 0:\n        print ('\\nNo contents for this language.')\n        continue\n    if len(topic_sentences) == 0:\n        print ('\\nNo topics for this language.')\n        continue\n\n    \n    print (\"\\nCalculating 'content' embeddings...\")\n\n    # encode all contents into our vector space. This takes about 5 minutes (depends on your GPU speed)\n    corpus_embeddings = bi_encoder.encode(content_sentences.title.to_list(), convert_to_tensor=True, show_progress_bar=False)\n    \n    if gpu_on:\n        corpus_embeddings = corpus_embeddings.cuda()\n        corpus_embeddings = util.normalize_embeddings(corpus_embeddings)\n    \n    print (\"\\nCalculating 'topic' embeddings...\")\n    \n    # Encode the topics using the bi-encoder\n    topic_embeddings = bi_encoder.encode(topic_sentences.title.to_list(), convert_to_tensor=True, show_progress_bar=False)\n    \n    if gpu_on:\n        topic_embeddings = topic_embeddings.cuda()\n        topic_embeddings = util.normalize_embeddings(topic_embeddings)\n    \n    print (\"\\nRunning matches...\")\n    \n    for i, (topic_embed, query) in enumerate(tqdm(zip (topic_embeddings, topic_sentences.title.to_list()), total=len(topic_sentences))):\n        results = search(query,\n                         topic_embed,\n                         corpus_embeddings, \n                         content_sentences.title.to_list(), \n                         content_sentences.id,\n                         cross_encoder,\n                         top_k)\n        matches[query] = results.values()\n        preds[topic_sentences.iloc[i].id] = results.keys()\nprint ('\\nEnd of calculating matches.')","metadata":{"execution":{"iopub.status.busy":"2023-03-09T09:56:06.464049Z","iopub.execute_input":"2023-03-09T09:56:06.465038Z","iopub.status.idle":"2023-03-09T10:00:38.326822Z","shell.execute_reply.started":"2023-03-09T09:56:06.464866Z","shell.execute_reply":"2023-03-09T10:00:38.325337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions in exploded format\npreds_df = pd.DataFrame(zip (list(preds.keys()), [list(vals) for vals in preds.values()]), columns=['id','content_ids']).explode('content_ids')\n\n# predictions in submissions format\ndf_preds_aux = pd.DataFrame(zip (list(preds.keys()), (' '.join(list(preds[key])) for key in preds.keys())), columns=['id','content_ids'])\n\npredicts_submission = pd.DataFrame(topics_df.id).merge (df_preds_aux, how ='left', on = 'id')\npredicts_submission.rename(columns={'id':'topic_id'}, inplace=True)\npredicts_submission.fillna(' ', inplace=True)\n\npredicts_submission\n#predicts_submission.content_ids.apply (lambda x: len(x.split()))","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:00:45.040191Z","iopub.execute_input":"2023-03-09T10:00:45.040629Z","iopub.status.idle":"2023-03-09T10:00:45.071503Z","shell.execute_reply.started":"2023-03-09T10:00:45.040591Z","shell.execute_reply":"2023-03-09T10:00:45.069679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model evaluation","metadata":{}},{"cell_type":"code","source":"scoring = True\nif scoring:\n    score = calculate_F2score(predicts_submission.sort_values('topic_id'), correlations_df)\n    if score is not None:\n        display(score)\n        print ('F2 mean score:', score.f2.mean())\n        print ('Correct predictions:', score.tp.sum())\n        print ('Topics to match:', len(topics_df))\n        print ('Contents to match:', len(corr_df.merge (topics_df.id, how = 'inner', left_on='topic_id', right_on='id')))","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:00:52.995902Z","iopub.execute_input":"2023-03-09T10:00:52.996539Z","iopub.status.idle":"2023-03-09T10:00:53.043436Z","shell.execute_reply.started":"2023-03-09T10:00:52.996489Z","shell.execute_reply":"2023-03-09T10:00:53.041785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submissions","metadata":{}},{"cell_type":"code","source":"#predicts_submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T08:59:35.277512Z","iopub.execute_input":"2023-03-09T08:59:35.278187Z","iopub.status.idle":"2023-03-09T08:59:35.282998Z","shell.execute_reply.started":"2023-03-09T08:59:35.278151Z","shell.execute_reply":"2023-03-09T08:59:35.281698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning wrong matches with LGBM","metadata":{}},{"cell_type":"code","source":"clean_wrong_matches = False","metadata":{"execution":{"iopub.status.busy":"2023-03-09T09:10:09.272142Z","iopub.execute_input":"2023-03-09T09:10:09.27263Z","iopub.status.idle":"2023-03-09T09:10:09.279001Z","shell.execute_reply.started":"2023-03-09T09:10:09.272585Z","shell.execute_reply":"2023-03-09T09:10:09.277951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calculating features 'X_test' for predictions","metadata":{}},{"cell_type":"code","source":"def categorize_features (X: pd.DataFrame):\n    obj_feat = list(X.loc[:, X.dtypes == 'object'].columns.values)\n    for feature in obj_feat:\n        X[feature] = pd.Series(X[feature], dtype=\"category\")\n    return X\n    \ndef get_y_class (y):\n    return (y > 0.5).astype(\"bool\")","metadata":{"execution":{"iopub.status.busy":"2023-03-09T09:10:09.84902Z","iopub.execute_input":"2023-03-09T09:10:09.85011Z","iopub.status.idle":"2023-03-09T09:10:09.856407Z","shell.execute_reply.started":"2023-03-09T09:10:09.850056Z","shell.execute_reply":"2023-03-09T09:10:09.855151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if clean_wrong_matches:\n    X_test = preds_df.merge (topic_features, how='inner', on='id')\n    X_test.drop (columns = ['language', 'has_content', 'description'], inplace=True)\n    X_test.rename (columns = {'id': 'topic_id', 'title':'topic_title'}, inplace=True)\n    X_test = X_test.merge (content_features, how='inner', left_on='content_ids', right_on='id')\n    X_test.drop (columns = ['title', 'description', 'text', 'id'], inplace=True)\n    X_test = categorize_features (X_test)\n    display (X_test, X_test.dtypes)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T09:10:10.176613Z","iopub.execute_input":"2023-03-09T09:10:10.177326Z","iopub.status.idle":"2023-03-09T09:10:10.254997Z","shell.execute_reply.started":"2023-03-09T09:10:10.177288Z","shell.execute_reply":"2023-03-09T09:10:10.253836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#corr_df\n#preds_df = preds_df.merge (corr_df, how='left', on = 'content_ids')\n#preds_df['match'] = (preds_df.id == preds_df.topic_id)\n#preds_df","metadata":{"execution":{"iopub.status.busy":"2023-03-09T09:10:10.544982Z","iopub.execute_input":"2023-03-09T09:10:10.54616Z","iopub.status.idle":"2023-03-09T09:10:10.551009Z","shell.execute_reply.started":"2023-03-09T09:10:10.546113Z","shell.execute_reply":"2023-03-09T09:10:10.549955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get model (load existing model or train model)","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\n\nif os.path.exists(private_files_path) and clean_wrong_matches:\n    for dirname, _, filenames in os.walk(private_files_path):\n        for filename in filenames:\n            filepath = os.path.join(dirname, filename)\n    model = lgb.Booster(model_file=filepath)\n    print ('Loaded model from :', filepath)\nelif clean_wrong_matches:\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import precision_score\n    import lightgbm as lgb\n    import optuna\n    import datetime\n        \n    objective = 'binary'\n    boosting_type = 'dart'\n\n    def objective_lgbm(trial):\n    \n        param = {\n            'boosting_type': boosting_type,\n            'objective': objective,\n            'is_unbalance': 'true',\n            'metric': 'precision_score',\n            'pos_bagging_fraction': trial.suggest_float('pos_bagging_fraction',0.1,1),\n            'neg_bagging_fraction': trial.suggest_float('neg_bagging_fraction',0.1,1),\n            'num_leaves': trial.suggest_int('num_leaves', 10,60),\n            'max_depth': trial.suggest_int('max_depth', 10,60),\n            'min_split_gain': trial.suggest_float('min_split_gain',0.1,1),\n            'colsample_bytree': trial.suggest_float('colsample_bytree',0.1,1),\n            'reg_alpha' : trial.suggest_float('reg_alpha',0.1,10),\n            'reg_lambda': trial.suggest_float('reg_lambda',0.1,10),\n            'n_estimators': trial.suggest_int('n_estimators', 150,350),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01,0.1),\n            'verbosity': -1\n        }\n        num_boost_rounds = trial.suggest_int('num_boost_rounds', 50,400)\n        model = lgb.train(param, train_data, num_boost_rounds)\n        preds = model.predict(X_val)\n        pred_labels = np.rint(preds)\n        score = round(precision_score(y_val, pred_labels),4)\n        return score\n    \n    # building a dataframe 'mix_matches' with 50% true topic-content matches and 50% false topic-content matches\n    df_aux = preds_df.merge (corr_df, how='left', on='content_ids')\n    true_matches = df_aux[df_aux['id'] == df_aux['topic_id']]\n    true_matches_val = len(df_aux[df_aux['id'] == df_aux['topic_id']])\n    false_matches = df_aux[df_aux['id'] != df_aux['topic_id']].sample(n=samples)\n    mix_matches = pd.concat([true_matches, false_matches])\n    mix_matches['y'] = (mix_matches.id == mix_matches.topic_id)\n    mix_matches.drop (columns='topic_id', inplace=True)\n\n    # building X features and y target \n    y = mix_matches['y']\n    X = mix_matches.merge (topic_features, how='inner', on='id')\n    #X = X.merge (content_features, how='inner', left_on='content_ids', right_on='id')\n    X.drop (columns = ['language', 'has_content', 'description', 'y'], inplace=True)\n    X.rename (columns = {'id': 'topic_id', 'title': 'topic_title'}, inplace=True)\n    X = X.merge (content_features, how='inner', left_on='content_ids', right_on='id')\n    X.drop (columns = ['title', 'description', 'text', 'id'], inplace=True)\n\n    # preparing X and y for model: changing dtype to \"category\" and creating train/val sets\n    X = categorize_features (X)\n\n    print(f\"Training features of shape {X.shape}\")\n    display (X)\n    print(f\"Training labels of shape {y.shape}\")\n    display (y)\n    \n    X_train, X_val, y_train, y_val = train_test_split (X, y, test_size = 0.3, random_state=42)\n\n    train_data = lgb.Dataset(X_train, label = y_train)\n\n    # get best hyperparameters\n    study_lgbm = optuna.create_study(direction = 'maximize',study_name = \"LGBM\")\n    study_lgbm.optimize(objective_lgbm, n_trials=50)\n\n    trial_lgbm = study_lgbm.best_trial\n    print(\"Model Accuracy --> \",trial_lgbm.value)\n    print(\"Model's Best parameters --> \",trial_lgbm.params)\n\n    # fit model and get score\n    num_boost_rounds = trial_lgbm.params['num_boost_rounds']\n    del trial_lgbm.params['num_boost_rounds']\n    trial_lgbm.params['boosting_type'] = boosting_type\n    trial_lgbm.params['objective'] = objective\n    trial_lgbm.params['is_unbalance'] = True\n    trial_lgbm.params['verbosity'] = -1\n    print(\"Using parameters --> \",trial_lgbm.params)\n    model = lgb.train(trial_lgbm.params, train_data, num_boost_rounds)\n    pred_model = model.predict(X_val)\n    pred_model = get_y_class (pred_model)\n    score = precision_score(y_val, pred_model)\n    print('\\nLightGBM Model accuracy score: {0:0.4f}'.format(score))\n    from sklearn.metrics import confusion_matrix\n    print('\\nConfusion Matrix : \\n' + str(confusion_matrix(y_val,pred_model)))\n    \n    # save model\n    date = datetime.datetime.now().strftime('%d%m%y-%H%M')\n    model_filename = f\"lgb-classifier-{date}.txt\"\n    model.save_model (model_filename)\n    print ('\\nSaved model as ', model_filename)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T09:10:11.272668Z","iopub.execute_input":"2023-03-09T09:10:11.273058Z","iopub.status.idle":"2023-03-09T09:10:11.314592Z","shell.execute_reply.started":"2023-03-09T09:10:11.273021Z","shell.execute_reply":"2023-03-09T09:10:11.313573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# New Predictions","metadata":{}},{"cell_type":"code","source":"if clean_wrong_matches:\n    # get predictions\n    preds_lgbm = model.predict(X_test)\n    preds_lgbm = get_y_class (preds_lgbm)\n    print ('\\nPredictions: ', preds_lgbm[-50:])\n    print (pd.DataFrame(preds_lgbm).value_counts())\n    #put predictions in final format\n    df_preds_aux = preds_df[preds_lgbm]\n    print (df_preds_aux.groupby('id').count())\n    df_preds_aux = df_preds_aux.groupby(['id']).apply (lambda x : ' '.join (x.iloc[:, 1])).reset_index()\n    df_preds_aux.rename (columns = {0: 'content_ids'}, inplace=True)\n    print (df_preds_aux)\n    predicts_submission = pd.DataFrame(topics_df.id).merge (df_preds_aux, how ='left', on = 'id')\n    predicts_submission.rename(columns={'id':'topic_id'}, inplace=True)\n    predicts_submission.fillna(' ', inplace=True)\n\n    display(predicts_submission)\n    \n    # submissions\n    predicts_submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T09:10:11.93353Z","iopub.execute_input":"2023-03-09T09:10:11.934275Z","iopub.status.idle":"2023-03-09T09:10:11.985343Z","shell.execute_reply.started":"2023-03-09T09:10:11.934236Z","shell.execute_reply":"2023-03-09T09:10:11.983956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicts_submission.content_ids.apply (lambda x: len(x.split()))","metadata":{"execution":{"iopub.status.busy":"2023-03-09T09:10:14.491336Z","iopub.execute_input":"2023-03-09T09:10:14.491719Z","iopub.status.idle":"2023-03-09T09:10:14.501704Z","shell.execute_reply.started":"2023-03-09T09:10:14.491684Z","shell.execute_reply":"2023-03-09T09:10:14.500502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Score","metadata":{}},{"cell_type":"code","source":"if clean_wrong_matches and scoring:\n    score = calculate_F2score(predicts_submission.sort_values('topic_id'), correlations_df)\n    if score is not None:\n        display(score)\n        print ('F2 mean score:', score.f2.mean())\n        print ('Correct predictions:', score.tp.sum())\n        print ('Topics to match:', len(topics_df))\n        print ('False positives:', score.fp.sum())\n        print ('False negatives:', score.fn.sum())\n        print ('Contents to match:', len(corr_df.merge (topics_df.id, how = 'inner', left_on='topic_id', right_on='id')))","metadata":{"execution":{"iopub.status.busy":"2023-03-09T09:10:16.331272Z","iopub.execute_input":"2023-03-09T09:10:16.3323Z","iopub.status.idle":"2023-03-09T09:10:16.367388Z","shell.execute_reply.started":"2023-03-09T09:10:16.332252Z","shell.execute_reply":"2023-03-09T09:10:16.36595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicts_submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T09:12:31.103372Z","iopub.execute_input":"2023-03-09T09:12:31.103775Z","iopub.status.idle":"2023-03-09T09:12:31.110423Z","shell.execute_reply.started":"2023-03-09T09:12:31.103739Z","shell.execute_reply":"2023-03-09T09:12:31.10933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}